<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Tridev — RAG Resume Chat</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <header>
    <h1>Tridev — Chat with my Résumé</h1>
    <div id="status">
      <span id="embed-status">Embeddings: loading…</span>
      <span id="llm-status">LLM: initializing…</span>
    </div>
    <p class="note">
      100% in your browser. First load downloads small models—give it a moment.
      If your device doesn’t support WebGPU, it will fall back to retrieval-only answers.
    </p>
  </header>

  <main>
    <section id="chat">
      <div id="messages" aria-live="polite"></div>
      <form id="composer">
        <input id="prompt" type="text" placeholder="Ask me anything about my experience, skills, projects…" autocomplete="off" />
        <button id="send" type="submit">Send</button>
      </form>
    </section>

    <aside id="sidebar">
      <h2>Sample questions</h2>
      <ul>
        <li class="suggest">What did you build at HERE for lane-level guidance?</li>
        <li class="suggest">Summarize my RAG stack and tooling.</li>
        <li class="suggest">Which projects show medical imaging work?</li>
        <li class="suggest">What quantization/pruning wins did I get?</li>
      </ul>

      <details>
        <summary>About this demo</summary>
        <ul>
          <li>Embeddings: <code>all-MiniLM-L6-v2</code> via Transformers.js</li>
          <li>Vector search: cosine similarity (normalized vectors)</li>
          <li>LLM (in-browser): WebLLM (small model, WebGPU)</li>
          <li>No servers, no API keys, static hosting via GitHub Pages</li>
        </ul>
      </details>
    </aside>
  </main>

  <footer>
    <small>
      Built for demo purposes. Answers are grounded in the résumé snippets shown as sources.
    </small>
  </footer>

  <!-- App code -->
  <script type="module" src="app.js"></script>
</body>
</html>
